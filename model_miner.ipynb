{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine jsons from HF and paginate. At the moment (may 2024) it's only 6 pages, causing neglectable traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Pages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:03<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 Models mined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "import panel as pn\n",
    "import json \n",
    "tqdm.pandas()\n",
    "\n",
    "# Base URL for the API\n",
    "base_url = \"https://huggingface.co/models-json?other=feature-extraction&library=transformers.js&sort=trending&numItemsPerPage=50\" # attention: https://huggingface.co/posts/do-me/362814004058611\n",
    "\n",
    "# List to store all models\n",
    "all_models = []\n",
    "\n",
    "# Total number of pages to fetch\n",
    "total_pages = 12 # adding more pages here for the future, should be raised once there are more than 300 models\n",
    "\n",
    "# Use tqdm to show progress\n",
    "for page_number in tqdm(range(0, total_pages + 1), desc=\"Fetching Pages\"):\n",
    "    # Construct the full URL for the current page\n",
    "    url = f\"{base_url}&p={page_number}\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON data from the response\n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Extract the models from the current page's data\n",
    "        page_models = json_data['models']\n",
    "        \n",
    "        # Append the models to the list\n",
    "        all_models.extend(page_models)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from page {page_number}. Status code: {response.status_code}\")\n",
    "\n",
    "print(all_models.__len__(), \"Models mined\")\n",
    "\n",
    "df = pd.DataFrame(all_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each of the models have a look at the onnx file sizes. Must request each page once unfortunately as it's not in the model's json\n",
    "Takes not more than 1.5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_size_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all 'a' tags with the specified title attribute\n",
    "            a_tags = soup.find_all('a', title=\"Download file\")\n",
    "            model_sizes = []  # Initialize a list to store sizes of model files\n",
    "\n",
    "            for a_tag in a_tags:\n",
    "                file_name_tag = a_tag.find_previous_sibling('div').find('span')\n",
    "                if not file_name_tag:  # Skip if there's no 'span' tag\n",
    "                    continue\n",
    "                file_name = file_name_tag.text.strip()\n",
    "                if file_name.endswith(\".onnx\"):\n",
    "                    if file_name.startswith(\"model\"):\n",
    "                        size = a_tag.text.strip().split(\"\\n\")[0]\n",
    "                        model_sizes.append(size)\n",
    "                    else: # only if there is no normal model\n",
    "                        if file_name.startswith((\"decoder\", \"encoder\")):\n",
    "                            size = a_tag.text.strip().split(\"\\n\")[0]\n",
    "                            model_sizes.append(size)\n",
    "                \n",
    "            if model_sizes:\n",
    "                return model_sizes\n",
    "            else:\n",
    "                return \"\"\n",
    "        else:\n",
    "            return \"\"#f\"HTTP Status Code: {response.status_code}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request error: {e}\"\n",
    "    \n",
    "# extract_size_from_url(\"https://huggingface.co/Xenova/instructor-large/tree/main/onnx\") # test\n",
    "\n",
    "def scrape_sizes(model):\n",
    "    sizes = extract_size_from_url(f\"https://huggingface.co/{model}/tree/main/onnx\")\n",
    "    sizes = [i.replace(\" \",\"\") for i in sizes]\n",
    "    sizes = \" | \".join(sizes)\n",
    "    return sizes\n",
    "\n",
    "# scrape_sizes(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 159/159 [01:30<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "df[\"sizes\"] = df[\"id\"].progress_apply(scrape_sizes) # 1 min 19s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove models that are not currently working (but have the transformers.js and feature-extraction tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing following models from dataset: \n",
      " 131                                       Severian/nomic\n",
      "143                     ChristianAzinn/uae-large-v1-gguf\n",
      "144             ChristianAzinn/mxbai-embed-large-v1-gguf\n",
      "147         ChristianAzinn/snowflake-arctic-embed-l-gguf\n",
      "148    ChristianAzinn/snowflake-arctic-embed-m-long-GGUF\n",
      "149         ChristianAzinn/snowflake-arctic-embed-m-gguf\n",
      "150         ChristianAzinn/snowflake-arctic-embed-s-gguf\n",
      "151        ChristianAzinn/snowflake-arctic-embed-xs-gguf\n",
      "152                                 Xenova/mobileclip_s0\n",
      "153                                 Xenova/mobileclip_s1\n",
      "154                                 Xenova/mobileclip_s2\n",
      "155                                  Xenova/mobileclip_b\n",
      "156                                Xenova/mobileclip_blt\n",
      "Name: id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing following models from dataset: \\n\",df[df[\"sizes\"] == \"\"].id)\n",
    "df = df[df[\"sizes\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add min and max onnx file size for sorting. Must be converted from different units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Conversion dictionary\n",
    "size_conversion = {'Byt': 1, 'Bytes': 1, 'kB': 1024, 'MB': 1024**2, 'GB': 1024**3}\n",
    "\n",
    "# Conversion function\n",
    "def size_to_bytes(size_str):\n",
    "    # Use regex to find the number and unit\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)\\s*(Byt|Bytes|kB|MB|GB)', size_str)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid size format: {size_str}\")\n",
    "    size_value = float(match.group(1))\n",
    "    size_unit = match.group(3)\n",
    "    return size_value * size_conversion[size_unit]\n",
    "\n",
    "# Parsing and conversion function\n",
    "def parse_and_find_min_max(sizes_str):\n",
    "    sizes_list = sizes_str.split(' | ')\n",
    "    sizes_bytes = [size_to_bytes(s) for s in sizes_list]\n",
    "    return min(sizes_bytes), max(sizes_bytes)\n",
    "\n",
    "# Apply the function and create new columns\n",
    "# Assuming 'df' is a pandas DataFrame and 'sizes' is a column in that DataFrame\n",
    "df['min_size'], df['max_size'] = zip(*df['sizes'].apply(parse_and_find_min_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values(\"min_size\", ascending=True).head(20) # sort as you please here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing all sizes below 50kB for the moment to filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"min_size\"] > 50000].reset_index(drop=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"trending\"] = df.index +1 # adding the trending column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "df[\"mined_date\"] = today # append to df so that one could easily concat dfs of different dates and do a groupby or similar, convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>authorData</th>\n",
       "      <th>downloads</th>\n",
       "      <th>gated</th>\n",
       "      <th>id</th>\n",
       "      <th>lastModified</th>\n",
       "      <th>likes</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>private</th>\n",
       "      <th>repoType</th>\n",
       "      <th>isLikedByUser</th>\n",
       "      <th>sizes</th>\n",
       "      <th>min_size</th>\n",
       "      <th>max_size</th>\n",
       "      <th>trending</th>\n",
       "      <th>mined_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mixedbread-ai</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>1755877</td>\n",
       "      <td>False</td>\n",
       "      <td>mixedbread-ai/mxbai-embed-large-v1</td>\n",
       "      <td>2024-04-18T23:20:55.000Z</td>\n",
       "      <td>309</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>1.34GB | 669MB | 337MB</td>\n",
       "      <td>353370112.0</td>\n",
       "      <td>1.438814e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nomic-ai</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>145806</td>\n",
       "      <td>False</td>\n",
       "      <td>nomic-ai/nomic-embed-text-v1.5</td>\n",
       "      <td>2024-05-03T02:21:07.000Z</td>\n",
       "      <td>171</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>548MB | 138MB</td>\n",
       "      <td>144703488.0</td>\n",
       "      <td>5.746196e+08</td>\n",
       "      <td>2</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nomic-ai</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>153516</td>\n",
       "      <td>False</td>\n",
       "      <td>nomic-ai/nomic-embed-text-v1</td>\n",
       "      <td>2024-05-03T02:21:44.000Z</td>\n",
       "      <td>365</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>548MB | 138MB</td>\n",
       "      <td>144703488.0</td>\n",
       "      <td>5.746196e+08</td>\n",
       "      <td>3</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alibaba-NLP</td>\n",
       "      <td>{'avatarUrl': 'https://www.gravatar.com/avatar...</td>\n",
       "      <td>75109</td>\n",
       "      <td>False</td>\n",
       "      <td>Alibaba-NLP/gte-large-en-v1.5</td>\n",
       "      <td>2024-04-26T13:51:26.000Z</td>\n",
       "      <td>56</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>1.75GB | 361MB | 873MB | 446MB | 387MB | 446MB...</td>\n",
       "      <td>378535936.0</td>\n",
       "      <td>1.879048e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WhereIsAI</td>\n",
       "      <td>{'avatarUrl': 'https://www.gravatar.com/avatar...</td>\n",
       "      <td>277613</td>\n",
       "      <td>False</td>\n",
       "      <td>WhereIsAI/UAE-Large-V1</td>\n",
       "      <td>2024-05-03T02:31:54.000Z</td>\n",
       "      <td>177</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>1.34GB | 669MB | 337MB</td>\n",
       "      <td>353370112.0</td>\n",
       "      <td>1.438814e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Snowflake</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>19315</td>\n",
       "      <td>False</td>\n",
       "      <td>Snowflake/snowflake-arctic-embed-m</td>\n",
       "      <td>2024-04-18T19:50:37.000Z</td>\n",
       "      <td>63</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>436MB | 144MB | 218MB | 110MB | 149MB | 110MB ...</td>\n",
       "      <td>115343360.0</td>\n",
       "      <td>4.571791e+08</td>\n",
       "      <td>6</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Snowflake</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>33257</td>\n",
       "      <td>False</td>\n",
       "      <td>Snowflake/snowflake-arctic-embed-l</td>\n",
       "      <td>2024-04-18T19:58:11.000Z</td>\n",
       "      <td>58</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>1.34GB | 299MB | 669MB | 337MB | 318MB | 337MB...</td>\n",
       "      <td>313524224.0</td>\n",
       "      <td>1.438814e+09</td>\n",
       "      <td>7</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Snowflake</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>7756</td>\n",
       "      <td>False</td>\n",
       "      <td>Snowflake/snowflake-arctic-embed-s</td>\n",
       "      <td>2024-04-18T19:58:21.000Z</td>\n",
       "      <td>9</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>133MB | 60.1MB | 66.7MB | 34MB | 61.4MB | 34MB...</td>\n",
       "      <td>35651584.0</td>\n",
       "      <td>1.394606e+08</td>\n",
       "      <td>8</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jinaai</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>15244</td>\n",
       "      <td>False</td>\n",
       "      <td>jinaai/jina-embeddings-v2-base-zh</td>\n",
       "      <td>2024-04-22T14:33:21.000Z</td>\n",
       "      <td>107</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>641MB | 321MB | 162MB</td>\n",
       "      <td>169869312.0</td>\n",
       "      <td>6.721372e+08</td>\n",
       "      <td>9</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Snowflake</td>\n",
       "      <td>{'avatarUrl': 'https://cdn-avatars.huggingface...</td>\n",
       "      <td>10108</td>\n",
       "      <td>False</td>\n",
       "      <td>Snowflake/snowflake-arctic-embed-m-long</td>\n",
       "      <td>2024-04-18T19:58:17.000Z</td>\n",
       "      <td>21</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>model</td>\n",
       "      <td>False</td>\n",
       "      <td>548MB | 158MB | 274MB | 138MB | 165MB | 138MB ...</td>\n",
       "      <td>144703488.0</td>\n",
       "      <td>5.746196e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>08-05-2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                         authorData  \\\n",
       "0  mixedbread-ai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "1       nomic-ai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "2       nomic-ai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "3    Alibaba-NLP  {'avatarUrl': 'https://www.gravatar.com/avatar...   \n",
       "4      WhereIsAI  {'avatarUrl': 'https://www.gravatar.com/avatar...   \n",
       "5      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "6      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "7      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "8         jinaai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "9      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
       "\n",
       "   downloads  gated                                       id  \\\n",
       "0    1755877  False       mixedbread-ai/mxbai-embed-large-v1   \n",
       "1     145806  False           nomic-ai/nomic-embed-text-v1.5   \n",
       "2     153516  False             nomic-ai/nomic-embed-text-v1   \n",
       "3      75109  False            Alibaba-NLP/gte-large-en-v1.5   \n",
       "4     277613  False                   WhereIsAI/UAE-Large-V1   \n",
       "5      19315  False       Snowflake/snowflake-arctic-embed-m   \n",
       "6      33257  False       Snowflake/snowflake-arctic-embed-l   \n",
       "7       7756  False       Snowflake/snowflake-arctic-embed-s   \n",
       "8      15244  False        jinaai/jina-embeddings-v2-base-zh   \n",
       "9      10108  False  Snowflake/snowflake-arctic-embed-m-long   \n",
       "\n",
       "               lastModified  likes         pipeline_tag  private repoType  \\\n",
       "0  2024-04-18T23:20:55.000Z    309   feature-extraction    False    model   \n",
       "1  2024-05-03T02:21:07.000Z    171  sentence-similarity    False    model   \n",
       "2  2024-05-03T02:21:44.000Z    365  sentence-similarity    False    model   \n",
       "3  2024-04-26T13:51:26.000Z     56  sentence-similarity    False    model   \n",
       "4  2024-05-03T02:31:54.000Z    177   feature-extraction    False    model   \n",
       "5  2024-04-18T19:50:37.000Z     63  sentence-similarity    False    model   \n",
       "6  2024-04-18T19:58:11.000Z     58  sentence-similarity    False    model   \n",
       "7  2024-04-18T19:58:21.000Z      9  sentence-similarity    False    model   \n",
       "8  2024-04-22T14:33:21.000Z    107   feature-extraction    False    model   \n",
       "9  2024-04-18T19:58:17.000Z     21  sentence-similarity    False    model   \n",
       "\n",
       "   isLikedByUser                                              sizes  \\\n",
       "0          False                             1.34GB | 669MB | 337MB   \n",
       "1          False                                      548MB | 138MB   \n",
       "2          False                                      548MB | 138MB   \n",
       "3          False  1.75GB | 361MB | 873MB | 446MB | 387MB | 446MB...   \n",
       "4          False                             1.34GB | 669MB | 337MB   \n",
       "5          False  436MB | 144MB | 218MB | 110MB | 149MB | 110MB ...   \n",
       "6          False  1.34GB | 299MB | 669MB | 337MB | 318MB | 337MB...   \n",
       "7          False  133MB | 60.1MB | 66.7MB | 34MB | 61.4MB | 34MB...   \n",
       "8          False                              641MB | 321MB | 162MB   \n",
       "9          False  548MB | 158MB | 274MB | 138MB | 165MB | 138MB ...   \n",
       "\n",
       "      min_size      max_size  trending  mined_date  \n",
       "0  353370112.0  1.438814e+09         1  08-05-2024  \n",
       "1  144703488.0  5.746196e+08         2  08-05-2024  \n",
       "2  144703488.0  5.746196e+08         3  08-05-2024  \n",
       "3  378535936.0  1.879048e+09         4  08-05-2024  \n",
       "4  353370112.0  1.438814e+09         5  08-05-2024  \n",
       "5  115343360.0  4.571791e+08         6  08-05-2024  \n",
       "6  313524224.0  1.438814e+09         7  08-05-2024  \n",
       "7   35651584.0  1.394606e+08         8  08-05-2024  \n",
       "8  169869312.0  6.721372e+08         9  08-05-2024  \n",
       "9  144703488.0  5.746196e+08        10  08-05-2024  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(f\"data/feature-extraction/transformersjs_{today}.xlsx\")\n",
    "df.to_parquet(f\"data/feature-extraction/transformersjs_{today}.parquet\")\n",
    "df.to_csv(f\"data/feature-extraction/transformersjs_{today}.csv\")\n",
    "df.to_json(f\"data/feature-extraction/transformersjs_{today}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To html options (ready to be pasted for SemanticFinder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "html_options = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Extracting relevant information from each row\n",
    "    author = row['author']\n",
    "    downloads = row['downloads']\n",
    "    likes = row['likes']\n",
    "    sizes = row['sizes']\n",
    "    id = row['id']\n",
    "\n",
    "    # Creating the option string\n",
    "    option_str = f'<option value=\"{id}\">{id} | üíæ{sizes} üì•{downloads} ‚ù§Ô∏è{likes}</option>'\n",
    "    \n",
    "    # Adding the option to the list\n",
    "    html_options.append(option_str)\n",
    "\n",
    "# Joining all options into a single string\n",
    "html_options_str = '\\n'.join(html_options)\n",
    "\n",
    "with open(f\"data/feature-extraction/transformersjs_html_options_{today}.html\", 'w') as file:\n",
    "    file.write(html_options_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To html table with filters/sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the editors for your columns\n",
    "tabulator_editors = {\n",
    "    'float': {'type': 'number', 'max': 10, 'step': 0.1},\n",
    "    'bool': {'type': 'tickCross', 'tristate': True, 'indeterminateValue': None},\n",
    "    'str': {'type': 'list', 'valuesLookup': True},\n",
    "}\n",
    "\n",
    "# Create the Tabulator widget with header filters\n",
    "header_filter_table = pn.widgets.Tabulator(\n",
    "    df, layout='fit_columns',\n",
    "    editors=tabulator_editors, header_filters=True\n",
    ")\n",
    "\n",
    "# Save the widget to HTML with header filters\n",
    "header_filter_table.save(f\"data/feature-extraction/transformersjs_{today}.html\")\n",
    "#df.to_html(index=False) # pandas has not sorting/filtering option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send ntfy notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the DataFrame into a list\n",
    "list_message = f\"Trending HuggingFace Embedding Models - {today}\\n\"\n",
    "list_message += f\"{df.__len__()} available for feature-extraction in transformers.js:\\n\\n\"\n",
    "\n",
    "for index, row in df.head(10).iterrows():\n",
    "    list_message += f\"{index + 1}. {row['id']}, Likes: {row['likes']}, Downloads: {row['downloads']}\\n Sizes: {row['sizes']}\\n\\n\"\n",
    "\n",
    "list_message += f\"Meta data about all {df.__len__()} models can be downloaded on GitHub as csv, xlsx, json, parquet, html. Models can be downloaded from HuggingFace. Originally designed for SemanticFinder, a web app for in-browser semantic search where you can test all models without installing anything.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notification sent to feature_extraction_transformers_js_models_daily. Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and weekday\n",
    "current_date = datetime.now()\n",
    "current_day_of_week = current_date.weekday()\n",
    "current_day_of_month = current_date.day\n",
    "\n",
    "# Define the base URL for the ntfy.sh server\n",
    "base_url = \"https://ntfy.sh/\"\n",
    "\n",
    "# Prepare the actions as a list of dictionaries\n",
    "actions = [\n",
    "    {\"action\": \"view\", \"label\": \"GitHub\", \"url\": \"https://github.com/do-me/trending-huggingface-models\"},\n",
    "    {\"action\": \"view\", \"label\": \"HuggingFace\", \"url\": \"https://huggingface.co/models?library=transformers.js&other=feature-extraction&sort=trending\"},\n",
    "    {\"action\": \"view\", \"label\": \"SemanticFinder\", \"url\": \"https://do-me.github.io/SemanticFinder/\"}\n",
    "]\n",
    "\n",
    "# Define the channel names\n",
    "channels = {\n",
    "    \"daily\": \"feature_extraction_transformers_js_models_daily\",\n",
    "    \"weekly\": \"feature_extraction_transformers_js_models_weekly\",\n",
    "    \"monthly\": \"feature_extraction_transformers_js_models_monthly\"\n",
    "}\n",
    "\n",
    "# Function to send notification\n",
    "def send_notification(channel, message):\n",
    "    payload = {\n",
    "        \"topic\": channel,\n",
    "        \"message\": list_message,\n",
    "        \"actions\": actions\n",
    "    }\n",
    "    response = requests.post(base_url, json=payload)\n",
    "    print(f\"Notification sent to {channel}. Status Code: {response.status_code}\")\n",
    "\n",
    "# Send daily notification\n",
    "send_notification(channels[\"daily\"], \"Daily request message\")\n",
    "\n",
    "# Check if today is Monday (0 is Monday, 6 is Sunday) and send weekly notification\n",
    "if current_day_of_week == 0:\n",
    "    send_notification(channels[\"weekly\"], \"Weekly request message\")\n",
    "\n",
    "# Check if today is the first of the month and send monthly notification\n",
    "if current_day_of_month == 1:\n",
    "    send_notification(channels[\"monthly\"], \"Monthly request message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          author                                         authorData  \\\n",
      "0  mixedbread-ai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "1       nomic-ai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "2       nomic-ai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "3    Alibaba-NLP  {'avatarUrl': 'https://www.gravatar.com/avatar...   \n",
      "4      WhereIsAI  {'avatarUrl': 'https://www.gravatar.com/avatar...   \n",
      "5      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "6      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "7      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "8         jinaai  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "9      Snowflake  {'avatarUrl': 'https://cdn-avatars.huggingface...   \n",
      "\n",
      "   downloads  gated                                       id  \\\n",
      "0    1755877  False       mixedbread-ai/mxbai-embed-large-v1   \n",
      "1     145806  False           nomic-ai/nomic-embed-text-v1.5   \n",
      "2     153516  False             nomic-ai/nomic-embed-text-v1   \n",
      "3      75109  False            Alibaba-NLP/gte-large-en-v1.5   \n",
      "4     277613  False                   WhereIsAI/UAE-Large-V1   \n",
      "5      19315  False       Snowflake/snowflake-arctic-embed-m   \n",
      "6      33257  False       Snowflake/snowflake-arctic-embed-l   \n",
      "7       7756  False       Snowflake/snowflake-arctic-embed-s   \n",
      "8      15244  False        jinaai/jina-embeddings-v2-base-zh   \n",
      "9      10108  False  Snowflake/snowflake-arctic-embed-m-long   \n",
      "\n",
      "               lastModified  likes         pipeline_tag  private repoType  \\\n",
      "0  2024-04-18T23:20:55.000Z    309   feature-extraction    False    model   \n",
      "1  2024-05-03T02:21:07.000Z    171  sentence-similarity    False    model   \n",
      "2  2024-05-03T02:21:44.000Z    365  sentence-similarity    False    model   \n",
      "3  2024-04-26T13:51:26.000Z     56  sentence-similarity    False    model   \n",
      "4  2024-05-03T02:31:54.000Z    177   feature-extraction    False    model   \n",
      "5  2024-04-18T19:50:37.000Z     63  sentence-similarity    False    model   \n",
      "6  2024-04-18T19:58:11.000Z     58  sentence-similarity    False    model   \n",
      "7  2024-04-18T19:58:21.000Z      9  sentence-similarity    False    model   \n",
      "8  2024-04-22T14:33:21.000Z    107   feature-extraction    False    model   \n",
      "9  2024-04-18T19:58:17.000Z     21  sentence-similarity    False    model   \n",
      "\n",
      "   isLikedByUser                                              sizes  \\\n",
      "0          False                             1.34GB | 669MB | 337MB   \n",
      "1          False                                      548MB | 138MB   \n",
      "2          False                                      548MB | 138MB   \n",
      "3          False  1.75GB | 361MB | 873MB | 446MB | 387MB | 446MB...   \n",
      "4          False                             1.34GB | 669MB | 337MB   \n",
      "5          False  436MB | 144MB | 218MB | 110MB | 149MB | 110MB ...   \n",
      "6          False  1.34GB | 299MB | 669MB | 337MB | 318MB | 337MB...   \n",
      "7          False  133MB | 60.1MB | 66.7MB | 34MB | 61.4MB | 34MB...   \n",
      "8          False                              641MB | 321MB | 162MB   \n",
      "9          False  548MB | 158MB | 274MB | 138MB | 165MB | 138MB ...   \n",
      "\n",
      "      min_size      max_size  trending  mined_date  \n",
      "0  353370112.0  1.438814e+09         1  08-05-2024  \n",
      "1  144703488.0  5.746196e+08         2  08-05-2024  \n",
      "2  144703488.0  5.746196e+08         3  08-05-2024  \n",
      "3  378535936.0  1.879048e+09         4  08-05-2024  \n",
      "4  353370112.0  1.438814e+09         5  08-05-2024  \n",
      "5  115343360.0  4.571791e+08         6  08-05-2024  \n",
      "6  313524224.0  1.438814e+09         7  08-05-2024  \n",
      "7   35651584.0  1.394606e+08         8  08-05-2024  \n",
      "8  169869312.0  6.721372e+08         9  08-05-2024  \n",
      "9  144703488.0  5.746196e+08        10  08-05-2024  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
